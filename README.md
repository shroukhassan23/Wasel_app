This project is a mobile application that translates sign language into written text in real-time using a machine learning model.
The app opens the camera and detects hand gestures (representing letters and numbers) using a pre-trained TensorFlow Lite model.
It supports both Arabic and English sign language alphabets and numbers. 
The detected characters are displayed on the screen and accumulated as words or sentences. 
Users can then review, copy, listen to, or share the translated text. 
This solution aims to enhance communication between deaf or hard-of-hearing individuals and the wider community through accessible mobile technology.
